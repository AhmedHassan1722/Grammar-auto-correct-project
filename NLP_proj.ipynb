{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e260435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "name\n",
      "be\n",
      "abdullah\n",
      "i\n",
      "can\n",
      "paly\n",
      "fotbal\n",
      "iyears\n",
      "i\n",
      "studyis\n",
      "math\n",
      "in\n",
      "unviersy\n",
      "ones\n",
      "of\n",
      "weeks\n",
      "name abdullah paly fotbal iyears studyis math unviersy ones weeks\n",
      "['name', 'abdullah', 'paly', 'fotbal', 'iyears', 'studyis', 'math', 'unviersy', 'ones', 'weeks']\n",
      "my name was abdullah , I can , play fotbal , i19years ,i studio math in universe ones of weeks\n",
      "my name was abdullah , I can , pale fatal , i19years ,i study path in universe ones of weeks\n",
      "My name was Abdullah, I can, play football, i19years, i study is math in universe ones of weeks\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#files = gutenberg.fileids()\n",
    "#print(files)\n",
    "\n",
    "text='my name was abdullah , I can , paly fotbal , i19years ,i studyis math in unviersy ones of weeks'\n",
    "#text= gutenberg.raw(files[0])\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # تحويل النص إلى حروف صغيرة\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # إزالة الأرقام\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # إزالة علامات الترقيم\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # تقسيم النص إلى جمل وكلمات\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    # إزالة الـ stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if not word.lower() in stop_words]\n",
    "    \n",
    "    #stemming\n",
    "    \n",
    "    #for word in words:\n",
    "     #   print(word,\"--->\",snowball.stem(word))\n",
    "    \n",
    "    \n",
    "    #Lemmatization\n",
    "    for word in words :\n",
    "        print(lemmatizer.lemmatize(word,'v'))\n",
    "\n",
    "\n",
    "    print(' '.join(filtered_words))    \n",
    "   \n",
    "    # إعادة النص المنظف\n",
    "    return filtered_words\n",
    "\n",
    "print(preprocess_text(text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "# Define the function to correct the grammar\n",
    "def correct_grammar(text):\n",
    "    # Create an instance of the Speller object from the autocorrect library\n",
    "    spell = Speller()\n",
    "    # Split the input text into individual words\n",
    "    words = text.split()\n",
    "    # Use the Speller's built-in correction method to correct each word\n",
    "    corrected_words = [spell(word) for word in words]\n",
    "    # Join the corrected words back into a single string\n",
    "    corrected_text = \" \".join(corrected_words)\n",
    "    return corrected_text\n",
    "\n",
    "# Test the function with some sample text\n",
    "#text = \"He goed to the park yesterday.\"\n",
    "print(correct_grammar(text)) # Output: \"He good to the park yesterday.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define the function to correct the grammar\n",
    "def correct_grammar(text):\n",
    "    # Create a TextBlob object for the input text\n",
    "    blob = TextBlob(text)\n",
    "    # Use the correct() method to correct the grammar\n",
    "    corrected_text = str(blob.correct())\n",
    "    # Return the corrected text\n",
    "    return corrected_text\n",
    "\n",
    "# Test the function with some sample text\n",
    "#text = \"He goed to the park yesterday.\"\n",
    "print(correct_grammar(text)) # Output: \"He went to the park yesterday.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import language_tool_python\n",
    "\n",
    "# Load the language tool\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Define the function to correct the grammar\n",
    "def correct_grammar(text):\n",
    "    # Use the language tool to check for grammar errors\n",
    "    matches = tool.check(text)\n",
    "\n",
    "    # Replace errors with suggested corrections\n",
    "    for match in reversed(matches):\n",
    "        suggestions = match.replacements\n",
    "        if suggestions:\n",
    "            start = match.offset\n",
    "            end = match.offset + match.errorLength\n",
    "            text = text[:start] + suggestions[0] + text[end:]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Test the function with some sample text\n",
    "#text = \"I goed to the store. I is very happy.\"\n",
    "corrected_text = correct_grammar(text)\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66a094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
